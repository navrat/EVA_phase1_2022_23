# -*- coding: utf-8 -*-
"""model_py_testing_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LKejf6oBur7w0Aont_gx6KTLdrPjRDF9

## Convolution Model Template
- Define Model object
- Define Training module
- Parameterization to select normalization types
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import torchvision
from matplotlib import pyplot as plt
import numpy as np

def norm_fn(out_channels, norm_type = "BN", group = 4):
  ''' 
  Returns the normalization to be used based on user inputs.
  User allowed to provide one of 3 inputs: BN | LN | GN for batch layer and group normalization repsectively.
  Specifying groups is only relevant for group_normalization
  '''
  if norm_type == "BN":
    return nn.BatchNorm2d(out_channels)
  elif norm_type == "LN":
    return nn.GroupNorm(1,out_channels) # Put all channels into a single group (equivalent with LayerNorm as per Torch documentation for GroupNorm)
  elif norm_type == "GN":
    return nn.GroupNorm(group, out_channels)
  else:
    print("relevant inputs allowed: BN | LN | GN for batch layer and group normalization repsectively. Proceeding with default batch normalization")
    return nn.BatchNorm2d(out_channels)

class NNet(nn.Module):
  def __init__(self, dropout_val = 0.1, norm_type = "batch", regularization_type = None, group_GN = 4):
    super(NNet, self).__init__()
    self.dropout_val = dropout_val
    self.norm_type = norm_type
    self.group_GN = group_GN
    # Input + Conv block 1
    self.conv1 = nn.Sequential(
        nn.Conv2d(in_channels=1, out_channels = 16, kernel_size = (3,3), padding=0, bias = False),
        nn.ReLU(),
        norm_fn(16, self.norm_type, self.group_GN),
        nn.Dropout(self.dropout_val)
    ) # 16*26*26
    self.conv2 = nn.Sequential(
        nn.Conv2d(in_channels=16, out_channels = 16, kernel_size = (3,3), padding=0, bias = False),
        nn.ReLU(),
        norm_fn(16, self.norm_type, self.group_GN),
        nn.Dropout(self.dropout_val)
    ) # 16*24*24

    # Transition block 1
    self.conv3 = nn.Sequential(
        nn.Conv2d(in_channels=16, out_channels = 8, kernel_size = (1,1), padding=0, bias = False),
        nn.ReLU(),
        norm_fn(8, self.norm_type, self.group_GN),
        nn.Dropout(self.dropout_val)
    ) # 8*24*24
    self.pool1 = nn.Sequential(
        nn.MaxPool2d(2,2)
    ) # 8*12*12

    # Conv block 2
    self.conv4 = nn.Sequential(
        nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3,3), padding=0, bias = False),
        nn.ReLU(),
        norm_fn(16, self.norm_type, self.group_GN),
        nn.Dropout(self.dropout_val)
    ) # 16*10*10
    self.conv5 = nn.Sequential(
        nn.Conv2d(in_channels = 16, out_channels = 8, kernel_size = (3,3), padding=0, bias=False),
        nn.ReLU(),
        norm_fn(8, self.norm_type, self.group_GN),
        nn.Dropout(self.dropout_val)
    ) # 8*8*8
    self.conv6 = nn.Sequential(
        nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size = (3,3), padding=0, bias=False),
        nn.ReLU(),
        norm_fn(16, self.norm_type, self.group_GN),
        nn.Dropout(self.dropout_val)
    ) # 16*6*6
    self.conv7 = nn.Sequential(
        nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = (3,3), padding=0, bias=False),
        nn.ReLU(),
        norm_fn(16, self.norm_type, self.group_GN),
        nn.Dropout(self.dropout_val)
    ) # 16*4*4

    # Output block 
    self.gap = nn.Sequential(
        nn.AvgPool2d(kernel_size = 4)
    ) # 16*1*1
    self.conv8 = nn.Sequential(
        nn.Conv2d(in_channels=16, out_channels=10, kernel_size=(1, 1), padding=0, bias=False)
    ) # 10*1*1

  def forward(self, x):
    x = self.conv1(x)
    x = self.conv2(x)
    x = self.conv3(x)
    x = self.pool1(x)
    x = self.conv4(x)
    x = self.conv5(x)
    x = self.conv6(x)
    x = self.conv7(x)
    x = self.gap(x)
    x = self.conv8(x)
    x = x.view(-1,10)

    return F.log_softmax(x, dim=-1)

def model_viz(metrics_m1, metrics_m2, metrics_m3, train_val = ["training","validation"], metric = ["loss","accuracy"]):
  """
  Visualization of 3 models and their 4 sets of metrics (train/validation X loss/accuracy)
  Each metrics_mi is a list expected in the order of [train_loss,validation_loss, train_accuracy, validation_accuracy]
  Setting up for 3 models but can be generalized for a single metrics_m1 tensor or 3-d matrix for multiple models
  """
  import numpy as np
  fig, axs = plt.subplots(2,2, figsize = (18,10))
  for i in np.arange(0,2): # represents loss/accuracy
    for j in np.arange(0,2): # represents train/validation
      axs[i,j].plot(metrics_m1[i+j])
      axs[i,j].plot(metrics_m2[i+j])
      axs[i,j].plot(metrics_m3[i+j])
      axs[i,j].set_xlabel('# epochs')
      axs[i,j].set_ylabel(f"{train_val[j]} {metric[i]}")
      axs[i,j].legend(['Model with GN', 'model with LN','model with L1+BN'], loc = 'upper right')  
      axs[i,j].set_title(f"{train_val[j]} {metric[i]} across the 3 models")
  fig.savefig('./normalization_regularization_experiments.png')